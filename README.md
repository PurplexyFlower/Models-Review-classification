# PhD Models - Review Classification with QLoRA Fine-tuning

This repository contains the implementation and results of fine-tuning large language models for review classification using QLoRA (Quantized Low-Rank Adaptation) technique. The project focuses on binary classification of product reviews (Real vs Fake) using two different model architectures: FLAN-T5-Large and Stella.

## ğŸ¯ Project Overview

This research project demonstrates efficient fine-tuning of large language models for text classification tasks using consumer-grade hardware. By implementing QLoRA, we achieve comparable performance to full fine-tuning while using significantly less computational resources.

### Key Achievements
- **FLAN-T5-Large**: Achieved **97.03% F1 Score** on review classification
- **Stella Model**: Implemented custom classification architecture with QLoRA
- **Memory Efficiency**: Reduced VRAM requirements by ~75% compared to full fine-tuning
- **Training Time**: 4-6 hours on consumer GPU vs 20+ hours for traditional fine-tuning

## ğŸ“ Repository Structure

```
â”œâ”€â”€ data/                           # Datasets and embeddings
â”‚   â”œâ”€â”€ dataset/                    # Raw and processed review data
â”‚   â””â”€â”€ stella_embeddings/          # Pre-computed Stella embeddings
â”œâ”€â”€ models/                         # Fine-tuned model checkpoints
â”‚   â”œâ”€â”€ flan-t5-large-qlora-tuned/ # FLAN-T5 QLoRA model (BEST: 97.03% F1)
â”‚   â””â”€â”€ stella_qlora_finetuned_model/ # Stella QLoRA model
â”œâ”€â”€ notebooks/                      # Jupyter notebooks for experiments
â”‚   â”œâ”€â”€ flan-t5-large-qlora-tuned.ipynb    # FLAN-T5 fine-tuning notebook
â”‚   â”œâ”€â”€ Stella_Classification_Methods_GPU.ipynb # Stella GPU implementation
â”‚   â””â”€â”€ Stella_QLoRA_Fine_Tuning.ipynb     # Stella QLoRA fine-tuning
â”œâ”€â”€ scripts/                        # Python scripts
â”‚   â”œâ”€â”€ FLan-t5-large_FT_QLoRA.py  # FLAN-T5 training script
â”‚   â””â”€â”€ FT_Stella.py               # Stella training script
â”œâ”€â”€ outputs/                        # Model weights and test results
â”œâ”€â”€ documentation/                  # Additional documentation
â””â”€â”€ .venv/                         # Python virtual environment
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- NVIDIA GPU with 6-8GB VRAM (minimum)
- CUDA-compatible drivers

### Installation

1. Clone the repository:
```bash
git clone https://github.com/your-username/phd-models-review-classification.git
cd phd-models-review-classification
```

2. Set up the virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

### Running the Models

#### FLAN-T5-Large QLoRA (Recommended - Best Performance)
```bash
# Run the notebook
jupyter notebook notebooks/flan-t5-large-qlora-tuned.ipynb

# Or run the script
python scripts/FLan-t5-large_FT_QLoRA.py
```

#### Stella QLoRA
```bash
# Run the notebook
jupyter notebook notebooks/Stella_QLoRA_Fine_Tuning.ipynb

# Or run the script
python scripts/FT_Stella.py
```

## ğŸ† Model Performance

### FLAN-T5-Large-ReviewClassifier-QLoRA (Best Model)
- **F1 Score**: 97.03%
- **Training Time**: ~5 hours
- **Memory Usage**: ~8GB VRAM
- **Model Size**: ~1.2GB (adapters only)
- **Best Checkpoint**: `models/flan-t5-large-qlora-tuned/checkpoint-5460/`

### Stella Model
- **Architecture**: Custom classification head with Stella embeddings
- **Training Method**: QLoRA fine-tuning
- **Status**: Experimental implementation

## ğŸ”§ Technical Details

### QLoRA Configuration
- **Quantization**: 4-bit NF4 with double quantization
- **LoRA Rank**: 16
- **LoRA Alpha**: 32
- **LoRA Dropout**: 0.05
- **Target Modules**: Attention layers (q, k, v, o)

### Training Hyperparameters
- **Learning Rate**: 2e-4
- **Batch Size**: 2 per device (effective: 16 with gradient accumulation)
- **Optimizer**: paged_adamw_8bit
- **Scheduler**: Cosine annealing
- **Max Epochs**: 5 (early stopping at epoch 3)

### Hardware Requirements
- **Training**: NVIDIA GPU with 6-8GB VRAM minimum
- **Inference**: Any CUDA GPU with 2GB+ VRAM
- **System RAM**: 16GB+ recommended

## ğŸ“Š Dataset

The project uses a cleaned dataset of product reviews with binary labels:
- **Real Reviews**: Genuine customer feedback
- **Fake Reviews**: Artificially generated or manipulated reviews
- **Data Splits**: 72% training, 8% validation, 20% testing
- **Preprocessing**: Text cleaning, tokenization, stratified splitting

## ğŸ“ˆ Results and Analysis

### Training Progress (FLAN-T5)
- **Epoch 1**: F1 = 96.45%
- **Epoch 2**: F1 = 96.52%
- **Epoch 3**: F1 = 97.03% â† **Best Model**
- **Epochs 4-5**: No improvement (early stopping)

### Memory Efficiency
- **Traditional Fine-tuning**: 40+ GB VRAM required
- **QLoRA Fine-tuning**: 8GB VRAM sufficient
- **Reduction**: ~75% memory savings

### Environmental Impact
- **Training Time**: 75% reduction vs full fine-tuning
- **Energy Consumption**: Significantly lower carbon footprint
- **Accessibility**: Enables research on consumer hardware

## ğŸ”¬ Research Contributions

1. **Efficient Fine-tuning**: Demonstrated QLoRA effectiveness for text classification
2. **Consumer Hardware**: Proved high-quality results on limited resources
3. **Comparative Analysis**: Evaluated multiple model architectures
4. **Reproducible Research**: Complete code and documentation provided

## ğŸ“ Usage Examples

### Loading the Best Model
```python
from transformers import AutoTokenizer, AutoModel
from peft import PeftModel

# Load the best checkpoint
model_path = "models/flan-t5-large-qlora-tuned/checkpoint-5460"
base_model = "google/flan-t5-large"

tokenizer = AutoTokenizer.from_pretrained(base_model)
model = PeftModel.from_pretrained(base_model, model_path)

# Classify a review
text = "This product exceeded my expectations! Fast shipping and great quality."
inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
outputs = model(**inputs)
prediction = torch.argmax(outputs.logits, dim=-1)
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## ğŸ“„ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.

## ğŸ“š Citation

If you use this work in your research, please cite:

```bibtex
@misc{phd-models-review-classification,
  title={Efficient Review Classification using QLoRA Fine-tuning of Large Language Models},
  author={PhD Research Project},
  year={2025},
  howpublished={GitHub Repository},
  url={https://github.com/your-username/phd-models-review-classification}
}
```

## ğŸ™ Acknowledgments

- **Google**: For the FLAN-T5 base model
- **Hugging Face**: For transformers and PEFT libraries
- **Microsoft**: For BitsAndBytes quantization
- **Research Community**: For QLoRA methodology

## ğŸ“ Contact

For questions about this research, please open an issue or contact [your-email@domain.com].

---

**Note**: This is a research project focused on demonstrating efficient fine-tuning techniques. The models are trained for educational and research purposes.
